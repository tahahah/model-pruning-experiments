# -*- coding: utf-8 -*-
"""Torch-Pruning-Demo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TRvELQDNj9PwM-EERWbF3IQOyxZeDepp

# Torch-Pruning Demo

<img src="https://github.com/VainF/Torch-Pruning/raw/master/assets/intro.png" width=400></img>
"""

!pip install --upgrade torch_pruning

import torch
import torchvision
import torch_pruning as tp

"""## Play Around with Torchvision Models

### 1. Choose a model to prune
"""

model_dict = {
    'resnet50': torchvision.models.resnet50,
    'resnet18': torchvision.models.resnet18,
    'convnext': torchvision.models.convnext_base,
    'vgg_19_bn': torchvision.models.vgg19_bn,
    'regnet_x_1_6gf': torchvision.models.regnet_x_1_6gf,
    'efficientnet_b4': torchvision.models.efficientnet_b4,
    'densenet121': torchvision.models.densenet121,
    'vit_b_32': torchvision.models.vit_b_32,
    'mobilenet_v3_large': torchvision.models.mobilenet_v3_large,
    # Register your models here. This demo only covers classification models.
}

model = model_dict['convnext'](pretrained=True)

"""### 2. Prepare a pruner

Note: When we prune a model like ConvNext and ViT, torch-pruning will show a warning about unwrapped parameters. This warning is caused by `nn.Parameter` that does not belong to any standard layers such as `nn.Conv2d`, `nn.BatchNorm`. By default, Torch-Pruning will automatically prune the last non-singleton dim of these parameters. If you want to customize this behaviour, please provide an `unwrapped_parameters` list as the following example.
"""

imp = tp.importance.GroupNormImportance(p=2)

ignored_layers = []
for m in model.modules():
  if isinstance(m, torch.nn.Linear) and m.out_features == 1000: # ignore the classifier
    ignored_layers.append(m)

# Group dims by head for MultiheadAttention
import torch.nn as nn
channel_groups = {}
for m in model.modules():
    if isinstance(m, nn.MultiheadAttention):
        channel_groups[m] = m.num_heads

pruner = tp.pruner.GroupNormPruner(
    model = model,
    example_inputs = torch.randn(1,3,224,224),
    importance = imp,     # Importance Estimator
    global_pruning=False, # Please refer to Page 9 of https://www.cs.princeton.edu/courses/archive/spring21/cos598D/lectures/pruning.pdf
    pruning_ratio = 0.2,    # global sparsity for all layers
    #pruning_ratio_dict = {model.conv1: 0.2}, # manually set the sparsity of model.conv1
    iterative_steps = 1,  # number of steps to achieve the target ch_sparsity.
    ignored_layers = ignored_layers,        # ignore some layers such as the finall linear classifier
    channel_groups = channel_groups,  # round channels
    #unwrapped_parameters=[ (model.features[1][1].layer_scale, 0), (model.features[5][4].layer_scale, 0) ],
)

"""### 3. Prune the model"""

# Model size before pruning
base_macs, base_nparams = tp.utils.count_ops_and_params(model, torch.randn(1,3,224,224))
pruner.step()

# modify some inferece-related attributes if necessary
if isinstance(
    model, torchvision.models.vision_transformer.VisionTransformer
):  # ViT relies on the hidden_dim attribute for forwarding, so we have to modify this variable after pruning
    model.hidden_dim = model.conv_proj.out_channels

# Parameter & MACs Counter
pruned_macs, pruned_nparams = tp.utils.count_ops_and_params(model, torch.randn(1,3,224,224))
print("The pruned model:")
print(model)
print("Summary:")
print("Params: {:.2f} M => {:.2f} M".format(base_nparams/1e6, pruned_nparams/1e6))
print("MACs: {:.2f} G => {:.2f} G".format(base_macs/1e9, pruned_macs/1e9))

# Test Forward
output = model(torch.randn(1,3,224,224))
print("Output.shape: ", output.shape)

# Test Backward
loss = torch.nn.functional.cross_entropy(output, torch.randint(1, 1000, (1,)))
loss.backward()

